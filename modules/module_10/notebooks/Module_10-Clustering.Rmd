---
title: "Module 10 - Hierarchical & k-means clustering"
author: "Loyal Goff"
date: "2022-11-06"
output:
  html_document:
    toc: true
    toc_level: 3
    toc_float: true
    theme: yeti
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Import
```{r}
library(SummarizedExperiment)
library(tidyverse)

# Read in the data
data <- read.csv("https://github.com/gofflab/Quant_mol_neuro_2022/raw/main/modules/module_9/notebooks/GSE63482_Expression_matrix.tsv", sep = " ", header = T, row.names = 1)
```

```{r}
# Parse header into parameters
sample_metadata <- as.data.frame(stringr::str_split_fixed(colnames(data), "_", 2))
colnames(sample_metadata) <- c("Age", "CellType")
sample_metadata$SampleID <- colnames(data)
colData <- DataFrame(sample_metadata)

# Create Gene metadata
gene_metadata <- data.frame(geneName = rownames(data))
rownames(gene_metadata) <- rownames(data)
gene_metadata <- DataFrame(gene_metadata)

# Create a Summarized Experiment object
se <- SummarizedExperiment(
  assays = SimpleList(fpkm = as.matrix(data)),
  rowData = gene_metadata,
  colData = sample_metadata
)
assay(se,'logfpkm')<-log10(assay(se,'fpkm')+1)
```

# Clean/process data to cluster
```{r}
clustData<-assay(se,'logfpkm')
clustData<-clustData[rowMeans(clustData)>=1,]
```

# Hierarchical clustering
First we will need to compute a distance matrix of the pairwise distances across samples. To do this we will use the `dist()` function. `dist()` operates over rows, so if we want to calculate the distance between samples, we will have to transpose (`t()`) clustData.
```{r}
clustData.dist<-dist(t(clustData))
clustData.dist
```

```{r}
hclust(clustData.dist)

plot(hclust(clustData.dist),main="Euclidean:complete")
```


```{r}
clustData.dist<-dist(t(clustData),method="manhattan")

plot(hclust(clustData.dist),main="Manhattan:complete")
```

Partition by choosing k
```{r}
clustData.hclust<-hclust(clustData.dist)
plot(clustData.hclust)
clus3<-cutree(clustData.hclust,k=3)
clus3

```

Partition by fixing height
```{r}
clustData.hclust<-hclust(clustData.dist)
plot(clustData.hclust)
height<-1000
abline(h=height,lty="dashed",col="red")
clusH1000<-cutree(clustData.hclust,h=height)
clusH1000

```

# K-means
Let's try and identify groups of genes that behave similarly across the conditions. To do this, and to avoid the complication of absolute expression level (we're interested in 'patterns' across conditions), we will first center & scale the row(gene) data.
```{r}
nClusters<-12
clustData.scaled<-t(scale(t(clustData)))
clustData.kmeans<-kmeans(clustData.scaled,nClusters)
```

```{r}
clustData.plot<-clustData.scaled
clustData.plot<-as.data.frame(clustData.plot)
clustData.plot$gene<-rownames(clustData.plot)
clustData.plot$cluster<-clustData.kmeans$cluster
clustData.melt<-reshape2::melt(clustData.plot,id.vars=c("gene","cluster"))
clustData.melt<-merge(clustData.melt,colData(se),by.x="variable",by.y="SampleID")
clustData.melt<-as.data.frame(clustData.melt)
```

Summarise data across clusters with dplyr
```{r}
kmeans.summary <- clustData.melt %>%
  group_by(cluster,Age,CellType) %>%
  summarise(mean=mean(value),sd=sd(value))
```

```{r}
p<-ggplot(kmeans.summary,aes(x=Age,
                             y=mean,
                             ymax=mean+sd,
                             ymin=mean-sd
                             )) + 
  geom_point(aes(color=CellType)) +
  geom_line(aes(group=CellType,color=CellType)) +
  geom_hline(yintercept=0,linetype="dashed") + 
  #geom_ribbon(aes(group=CellType,color=CellType,fill=CellType),alpha = 0.2)
  theme_minimal()
p + facet_wrap('cluster',scales="free_y")
```

# Clustering QC
How do we determine that a clustering solution is optimal/effective?  In many cases, this requires us to test multiple clustering solutions using a given metric.

## Choosing a k
Methods such as k-means clustering which require an a priori choice of k can be evaluated by asking the question, 'What is the best k'.  In many cases, we can use some intuition to set a starting value for k, but we can also assess how well a specific clustering solution performs, and choose a k from a range of values based on this assessment.
### Elbow method
A naive (but reasonable) approach to determining the choice of k is to ask which k minimizes the 'Within-cluster-sum of Squared Errors' (WSS) across different choices of k. The WSS is the sum of the distances for each point to its cluster centroid. 

1) For each point, calculate the square of the distance between said point and its cluster centroid (Squared Error)
2) WSS is the sum of all of the squared errors for each point.

This is conveniently calculated for us and provided for each kmeans result in the `$tot.withinss` slot.

```{r}
clustData.kmeans$tot.withinss
```

We can use this value, for multiple choices of k, to assess the goodness of fit for each clustering solution. An 'optimal' k will minimize the WSS but not 'overfit' or add more clusters than needed to describe the different groups effectively.
To choose an optimal k, let's write a for loop to do this for us.

```{r}
kRange<-c(2:30)
wss<-c()
for (i in kRange){
  print(i)
  clust<-kmeans(clustData.scaled,i,iter.max = 100)
  wss<-c(wss,clust$tot.withinss)
}
```

Now that we have performed the clustering over a range of k-values, let's plot the wss trends and see where the elbow of the curve is located. We want to choose a k that minimizes the wss, but doesn't 'overfit' the data.

```{r}
plot(kRange,wss,type="l")
```

Ideally you want a clustering that has the properties of both internal cohesion (low _within_ cluster SS) and external separation (high _between_ cluster SS). We can calculate the external separation by measuring the Sum of the Square of the distances between cluster centroids
```{r}
clustData.kmeans$betweenss
```

The ratio of BSS / TSS (TSS = BSS + WSS) gives us another metric to evaluate the 'goodness' of a clustering solution as well 
```{r}
clustData.kmeans$betweenss/(clustData.kmeans$betweenss+clustData.kmeans$tot.withinss)
```

For this metric, an 'optimal' clustering solution would have a BSS/TSS ratio approaching 1.

### Silhouette method
The silhouette score is another metric used to measure how close each point in one cluster is to points in the neighboring clusters.

Silhouette Score = (b-a)/max(a,b) (Range: [-1:1])
where:
  - a = average intra-cluster distance i.e the average distance between each point within a cluster.
  - b = average inter-cluster distance i.e the average distance between all clusters.

```{r}
library(cluster)
D<-dist(clustData.scaled)
kmeans.sil<-silhouette(clustData.kmeans$cluster,D)
plot(kmeans.sil,border=NA,col=1:5)

silhouette_score <- function(k){
  km <- kmeans(clustData.scaled, centers = k, iter.max=100)
  ss <- silhouette(km$cluster, D)
  mean(ss[, 3])
}

kRange <- c(2:30)
avg_sil <- lapply(kRange, silhouette_score)
plot(kRange, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)


nClusters<-12
clustData.kmeans<-kmeans(clustData.scaled,nClusters,iter.max=100)
kmeans.sil<-silhouette(clustData.kmeans$cluster,D)
plot(kmeans.sil,border=NA,col=1:nClusters)

```

## UMAP on genes to visualize cluster relationships
```{r}
library(uwot)
genes.umap<-uwot::umap(clustData.scaled,verbose=T) # or uwot::umap()
plot(genes.umap,pch=20)
plot(genes.umap,
     col=clustData.kmeans$cluster,
     pch=20) #Run this several times....
```

# Session Information
```{r}
sessionInfo()
```

